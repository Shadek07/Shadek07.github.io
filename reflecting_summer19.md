I joined HDI lab in last Spring. Last spring passed by getting to know about the lab and the research topics. I became more interested in the lab after knowing that the research in the lab is being conducted at the cross section of Artificial general Intelligence, Machine Learning, Deep Neural Network and Reinforcement learning. My summer in the lab started with replicating world model paper. World model is a phenomenal paper on reinforcement learning environment training. It was not quite easy for me to reproduce world model experiments when I did not  have (I still do not) enough knowledge on Reinforcement learning and Application of Neural network. In one of our HDI lab seminar I confused popular PPO algorithm with world model concept. There were many underlying concepts before attempting to replicate: ml-agents, Open-AI gym, Variational autoencoder etc. Once I became familiar with all of those, replicating experiments became easier.

One of several of my plans in summer was to complete three courses on Reinforcement learning in Udemy. When I started coding for those courses, things became much clearer. 

After replicating world model, my next plan was to apply world model for Visual pushblock game. Visual pushblock is a game environment built by Unity3d company. First I stumbled upon on making executable visual pushblock game env in Unity3d. I took me couple of days to figure out why pushblock unity env was not behaving properly in python. Next I went deep into source code of world model to understand code and then change code to use unity3d environment for implementation. One component of this implementation is to train Variational autoencoder(VAE). After training VAE, I found out the vae training was successful. Reconstructed images was almost identical to original images. Then I made interactive demo of VAE using interactive source code of worldmodel. For this interactive demo, I had to write code in javascript, python and html. At some stage of making this demo, I had no clue how the demo is supposed to work. But gladly, after spending 2-3 days of hard work I was able to make interactive VAE [demo](https://stupefied-lumiere-eaad09.netlify.com). Then I finished other parts of the experiment.

My major plan for Fall 2019 is to compare multiple experiments of worldmodel with different settings, get deep understanding of go-explore system built by Uber-AI team and use that for Visual Pushblock game to see performance improvement. I also have plan to learn about Imitation and curriculum learning, .
